<!DOCTYPE html>
<html lang="fr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Les r√©seaux de neurones r√©currents : des RNN simples aux LSTM</title>
<script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
<style>
    *{margin:0;padding:0;box-sizing:border-box;}
    body{
        font-family:'Segoe UI',Arial,sans-serif;
        background:linear-gradient(135deg, #0d1117 0%, #1a1f2e 100%);
        color:#e5e7eb;line-height:1.8;min-height:100vh;
    }
    
    .container{
        max-width:1200px;margin:0 auto;padding:40px 20px;
    }
    
    .header{
        text-align:center;margin-bottom:50px;padding-bottom:30px;
        border-bottom:2px solid rgba(125,211,252,0.3);
    }
    
    h1{
        color:#7dd3fc;font-size:42px;margin-bottom:15px;
        font-weight:700;line-height:1.2;
    }
    
    .meta{
        color:#9ca3af;font-size:14px;margin-top:15px;
    }
    
    .meta span{
        margin:0 10px;
    }
    
    .back-link{
        display:inline-block;margin-bottom:30px;padding:10px 20px;
        background:rgba(125,211,252,0.1);color:#7dd3fc;
        text-decoration:none;border-radius:8px;transition:all 0.3s;
    }
    .back-link:hover{
        background:rgba(125,211,252,0.2);transform:translateX(-5px);
    }
    
    .content{
        background:rgba(26,31,46,0.6);padding:40px;border-radius:20px;
        box-shadow:0 10px 40px rgba(0,0,0,0.3);
    }
    
    h2{
        color:#7dd3fc;font-size:32px;margin-top:50px;margin-bottom:20px;
        padding-bottom:10px;border-bottom:2px solid rgba(125,211,252,0.2);
    }
    
    h3{
        color:#a78bfa;font-size:24px;margin-top:35px;margin-bottom:15px;
    }
    
    h4{
        color:#cbd5e1;font-size:20px;margin-top:25px;margin-bottom:12px;
    }
    
    p{
        margin:15px 0;color:#d1d5db;font-size:16px;
    }
    
    .highlight-box{
        background:rgba(59,130,246,0.1);padding:20px;border-radius:10px;
        border-left:4px solid #3b82f6;margin:25px 0;
    }
    
    .info-box{
        background:rgba(16,185,129,0.1);padding:20px;border-radius:10px;
        border-left:4px solid #10b981;margin:25px 0;
    }
    
    .warning-box{
        background:rgba(245,158,11,0.1);padding:20px;border-radius:10px;
        border-left:4px solid #f59e0b;margin:25px 0;
    }
    
    .formula{
        background:rgba(139,92,246,0.1);padding:15px;border-radius:8px;
        font-family:'Courier New',monospace;font-size:16px;margin:20px 0;
        text-align:center;color:#c4b5fd;border:1px solid rgba(139,92,246,0.3);
    }
    
    ul,ol{
        margin:15px 0 15px 30px;color:#d1d5db;
    }
    
    li{
        margin:10px 0;font-size:16px;
    }
    
    code{
        background:rgba(0,0,0,0.3);padding:3px 8px;border-radius:4px;
        font-family:'Courier New',monospace;color:#fbbf24;font-size:14px;
    }
    
    .code-block{
        background:#1e1e1e;padding:20px;border-radius:10px;margin:20px 0;
        border:1px solid rgba(125,211,252,0.2);overflow-x:auto;
    }
    
    .code-block code{
        color:#d4d4d4;font-size:14px;line-height:1.6;
    }
    
    .section-divider{
        height:2px;background:linear-gradient(90deg,transparent,rgba(125,211,252,0.5),transparent);
        margin:50px 0;
    }
    
    .visualization-box{
        background:rgba(30,41,59,0.6);padding:30px;border-radius:15px;
        margin:30px 0;border:2px solid rgba(125,211,252,0.2);
        box-shadow:0 5px 20px rgba(0,0,0,0.3);
    }
    
    .visualization-box h4{
        color:#7dd3fc;margin-bottom:20px;text-align:center;
    }
    
    .chart-container{
        position:relative;height:400px;width:100%;margin:20px 0;
        background:rgba(15,23,42,0.5);padding:20px;border-radius:10px;
    }
    
    .matrix-container{
        display:flex;justify-content:center;align-items:center;
        margin:30px 0;flex-wrap:wrap;gap:20px;
    }
    
    .matrix{
        background:rgba(30,41,59,0.8);padding:20px;border-radius:10px;
        border:2px solid rgba(125,211,252,0.3);
    }
    
    .matrix-label{
        text-align:center;color:#7dd3fc;font-weight:bold;
        margin-bottom:10px;font-size:18px;
    }
    
    .matrix-grid{
        display:grid;gap:8px;padding:10px;
    }
    
    .matrix-cell{
        background:rgba(59,130,246,0.2);padding:12px;border-radius:5px;
        text-align:center;color:#e5e7eb;font-weight:600;
        border:1px solid rgba(125,211,252,0.2);
        min-width:50px;
    }
    
    .dimension-label{
        color:#7dd3fc;font-size:14px;margin:5px 0;
        text-align:center;font-style:italic;
    }
    
    .architecture-diagram{
        background:rgba(15,23,42,0.7);padding:40px;border-radius:15px;
        margin:30px 0;position:relative;
        border:2px solid rgba(125,211,252,0.2);
    }
    
    .rnn-cell-diagram{
        display:flex;justify-content:space-around;align-items:center;
        flex-wrap:wrap;gap:30px;padding:30px;
    }
    
    .diagram-box{
        background:linear-gradient(135deg, rgba(59,130,246,0.2), rgba(139,92,246,0.2));
        padding:25px;border-radius:12px;min-width:150px;
        text-align:center;border:2px solid rgba(125,211,252,0.4);
        position:relative;
    }
    
    .diagram-box h5{
        color:#7dd3fc;margin-bottom:10px;font-size:16px;
    }
    
    .diagram-box .value{
        color:#c4b5fd;font-size:20px;font-weight:bold;
        font-family:'Courier New',monospace;
    }
    
    .diagram-arrow{
        color:#7dd3fc;font-size:32px;font-weight:bold;
    }
    
    .table-container{
        overflow-x:auto;margin:30px 0;
    }
    
    table{
        width:100%;border-collapse:collapse;
        background:rgba(30,41,59,0.8);border-radius:10px;
        overflow:hidden;
    }
    
    th{
        background:rgba(59,130,246,0.3);color:#7dd3fc;
        padding:15px;text-align:left;font-weight:600;
        border-bottom:2px solid rgba(125,211,252,0.3);
    }
    
    td{
        padding:12px 15px;border-bottom:1px solid rgba(125,211,252,0.1);
        color:#e5e7eb;
    }
    
    tr:hover{
        background:rgba(125,211,252,0.05);
    }
    
    .steps-list{
        list-style:none;margin-left:0;
    }
    
    .steps-list li{
        padding:15px;margin:15px 0;background:rgba(59,130,246,0.1);
        border-left:4px solid #3b82f6;border-radius:5px;
        position:relative;padding-left:50px;
    }
    
    .steps-list li::before{
        content:counter(step);counter-increment:step;
        position:absolute;left:15px;top:15px;
        background:#3b82f6;color:#fff;width:25px;height:25px;
        border-radius:50%;display:flex;align-items:center;justify-content:center;
        font-weight:bold;font-size:14px;
    }
    
    .steps-list{
        counter-reset:step;
    }
    
    strong{
        color:#e5e7eb;font-weight:600;
    }
    
    .tag{
        display:inline-block;padding:5px 12px;background:rgba(125,211,252,0.2);
        color:#7dd3fc;border-radius:15px;font-size:12px;margin:5px;
    }
    
    .svg-diagram{
        width:100%;height:auto;margin:20px 0;
    }
    
    .comparison-grid{
        display:grid;grid-template-columns:repeat(auto-fit, minmax(300px, 1fr));
        gap:20px;margin:30px 0;
    }
    
    .comparison-card{
        background:rgba(30,41,59,0.6);padding:20px;border-radius:10px;
        border:2px solid rgba(125,211,252,0.2);
    }
    
    .comparison-card h5{
        color:#7dd3fc;margin-bottom:15px;text-align:center;
        font-size:20px;
    }
</style>
</head>
<body>

<div class="container">
    <a href="index.html" class="back-link">‚Üê Retour √† l'accueil</a>
    
    <div class="header">
        <h1>üß† Les r√©seaux de neurones r√©currents : des RNN simples aux LSTM</h1>
        <div class="meta">
            <span>Par <strong>Sofiene Alouini, Sofia Calcagno</strong></span>
            <span>‚Ä¢</span>
            <span>21/10/2019</span>
        </div>
        <div style="margin-top:15px;">
            <span class="tag">Data & AI</span>
        </div>
    </div>
    
    <div class="content">
        <p>Les r√©seaux de neurones constituent aujourd'hui l'√©tat de l'art pour diverses t√¢ches d'apprentissage automatique. Ils sont tr√®s largement utilis√©s par exemple dans les domaines de la vision par ordinateur (classification d'images, d√©tection d'objets, segmentation‚Ä¶) et du traitement automatique du langage (traduction automatique, reconnaissance vocale, mod√®les de langage‚Ä¶).</p>
        
        <p>Dans un pr√©c√©dent article, nous avons utilis√© une classe particuli√®re de r√©seaux de neurones, les RNN : <strong>Recurrent Neural Networks</strong>. Cette famille de mod√®les, particuli√®rement adapt√©e aux donn√©es s√©quentielles, nous a permis de g√©n√©rer automatiquement, caract√®re par caract√®re, du texte compr√©hensible √† partir d'une s√©quence initiale. Nous vous avions alors promis de vous expliquer plus en d√©tail le fonctionnement des mod√®les en question.</p>
        
        <p>Le but de ce nouvel article est de d√©cortiquer deux variantes de RNN pour en expliquer le fonctionnement interne et d√©mythifier leur aspect "bo√Æte noire". Pour cela, nous utiliserons le m√™me cas d'usage de g√©n√©ration de texte que dans l'article pr√©c√©dent. Il nous servira de fil rouge tout au long de cet article.</p>
        
        <div class="section-divider"></div>
        
        <h2>Anatomie du RNN</h2>
        
        <h3>Architecture globale</h3>
        
        <p>Commen√ßons par rappeler le probl√®me de g√©n√©ration de texte. Afin de g√©n√©rer du texte, nous partons d'une s√©quence de caract√®res de taille fixe pour pr√©dire le caract√®re suivant.</p>
        
        <div class="visualization-box">
            <h4>üìä Dimensions des Donn√©es</h4>
            <div class="table-container">
                <table>
                    <thead>
                        <tr>
                            <th>Param√®tre</th>
                            <th>Notation</th>
                            <th>Valeur Exemple</th>
                            <th>Description</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>Nb √©chantillons</strong></td>
                            <td><code>N</code></td>
                            <td>Variable</td>
                            <td>Nombre de s√©quences d'entra√Ænement</td>
                        </tr>
                        <tr>
                            <td><strong>Taille s√©quence</strong></td>
                            <td><code>T</code></td>
                            <td>10</td>
                            <td>Nombre de caract√®res par s√©quence</td>
                        </tr>
                        <tr>
                            <td><strong>Nb variables</strong></td>
                            <td><code>M</code></td>
                            <td>30</td>
                            <td>Taille du vecteur one-hot par caract√®re</td>
                        </tr>
                        <tr>
                            <td><strong>Hidden size</strong></td>
                            <td><code>R</code></td>
                            <td>16</td>
                            <td>Taille de la repr√©sentation cach√©e</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>
        
        <p>Le pr√©-traitement d√©taill√© dans l'article pr√©c√©dent permet de repr√©senter num√©riquement le texte brut, le transformant en une matrice dont les dimensions sont <code>nb_√©chantillons (N) * taille_s√©quence (T) * nb_variables (M)</code> o√π :</p>
        
        <ul>
            <li><strong>nb_√©chantillons (N)</strong> = nombre de s√©quences d'entra√Ænement (taille du dataset)</li>
            <li><strong>taille_s√©quence (T)</strong> = nombre (fixe) de caract√®res par s√©quence (dimension temporelle). Dans l'article pr√©c√©dent, nous avions choisi d'entra√Æner le mod√®le sur des s√©quences de taille T=50. Le choix de T est empirique et il y a un compromis √† trouver : si T est petit, le mod√®le aura la "m√©moire courte". Si T est grand, l'entra√Ænement sera tr√®s long et pas toujours efficace. Pour une meilleure lisibilit√©, nous choisirons plut√¥t T=10 dans les illustrations suivantes.</li>
            <li><strong>nb_variables (M)</strong> = taille de la repr√©sentation vectorielle de chaque caract√®re. Nous avons choisi de limiter notre vocabulaire aux 26 lettres de l'alphabet + 4 caract√®res sp√©ciaux. Chaque caract√®re est donc repr√©sent√© par un vecteur de taille M=30 gr√¢ce au one-hot-encoding.</li>
        </ul>
        
        <p>En r√©sum√©, le mod√®le prend en entr√©e un tableau de <strong>N s√©quences, chacune de longueur T=10 caract√®res</strong>, o√π chaque caract√®re est un vecteur num√©rique de taille M=30.</p>
        
        <div class="info-box">
            <p><strong>Une couche de type RNN</strong> prend en entr√©e une s√©quence de ce tableau (donc une matrice de taille (T x M)) et retourne en sortie un vecteur de taille R qui est une repr√©sentation compress√©e de la totalit√© de la s√©quence de caract√®res. Encore une fois, R est un hyperparam√®tre √† choisir judicieusement. Pour l'exemple, on peut choisir R=16. C'est en quelque sorte l'√©quivalent du nombre de neurones dans une couche dense (ou "fully-connected").</p>
        </div>
        
        <div class="visualization-box">
            <h4>üîÑ Visualisation de la Transformation</h4>
            <div class="matrix-container">
                <div class="matrix">
                    <div class="matrix-label">Entr√©e : X</div>
                    <div class="dimension-label">T √ó M = 10 √ó 30</div>
                    <div class="matrix-grid" style="grid-template-columns: repeat(5, 1fr);">
                        <div class="matrix-cell">x‚ÇÅ</div>
                        <div class="matrix-cell">x‚ÇÇ</div>
                        <div class="matrix-cell">...</div>
                        <div class="matrix-cell">x‚Çâ</div>
                        <div class="matrix-cell">x‚ÇÅ‚ÇÄ</div>
                    </div>
                </div>
                <div style="color:#7dd3fc;font-size:32px;align-self:center;">‚Üí</div>
                <div class="matrix">
                    <div class="matrix-label">Sortie : h‚Çú</div>
                    <div class="dimension-label">R = 16</div>
                    <div class="matrix-grid" style="grid-template-columns: repeat(4, 1fr);">
                        <div class="matrix-cell">h‚ÇÅ</div>
                        <div class="matrix-cell">h‚ÇÇ</div>
                        <div class="matrix-cell">...</div>
                        <div class="matrix-cell">h‚ÇÅ‚ÇÜ</div>
                    </div>
                </div>
            </div>
        </div>
        
        <p>G√©n√©rer le caract√®re suivant consiste √† choisir le caract√®re le plus pertinent parmi les M=30 caract√®res possibles. Il faut donc que notre mod√®le produise une sortie de taille M=30. Ce vecteur de sortie associe alors √† chaque caract√®re une probabilit√© d'√™tre le suivant dans la s√©quence, √©tant donn√© les T caract√®res pr√©c√©dents.</p>
        
        <p>Pour obtenir cette distribution de probabilit√©, on empile une couche de neurones "classique" (appel√©e √©galement couche dense) de taille M=30 √† la suite de la couche RNN et on utilise une fonction d'activation softmax. Consid√©rons l'exemple ci-dessous: la s√©quence d'entr√©e √©tant "il fait be", la probabilit√© de sortie la plus √©lev√©e pourrait correspondre √† la lettre "a", dans l'optique de g√©n√©rer la phrase "il fait beau".</p>
        
        <p>D√©taillons maintenant ce qui se passe √† l'int√©rieur-m√™me de la couche RNN. Une couche RNN est une succession de T cellules. Chaque cellule a deux entr√©es :</p>
        
        <ul>
            <li>l'√©l√©ment de la s√©quence lui correspondant (en version one-hot) : la t√®me cellule est associ√©e au t√®me caract√®re de la s√©quence</li>
            <li>le vecteur en sortie de la cellule pr√©c√©dente. La premi√®re cellule, qui n'a pas d'ant√©c√©dent, prend alors un vecteur initialis√© al√©atoirement</li>
        </ul>
        
        <div class="architecture-diagram">
            <h4 style="text-align:center;margin-bottom:30px;">üèóÔ∏è Architecture d'une Couche RNN (T=10)</h4>
            <div style="display:flex;justify-content:space-between;align-items:center;flex-wrap:wrap;gap:15px;">
                <div class="diagram-box">
                    <h5>h‚ÇÄ</h5>
                    <div class="value">0.0</div>
                </div>
                <div class="diagram-arrow">‚Üí</div>
                <div class="diagram-box">
                    <h5>Cellule 1</h5>
                    <div class="value">x‚ÇÅ</div>
                </div>
                <div class="diagram-arrow">‚Üí</div>
                <div class="diagram-box">
                    <h5>Cellule 2</h5>
                    <div class="value">x‚ÇÇ</div>
                </div>
                <div class="diagram-arrow">‚Üí</div>
                <div style="color:#9ca3af;">...</div>
                <div class="diagram-arrow">‚Üí</div>
                <div class="diagram-box">
                    <h5>Cellule 10</h5>
                    <div class="value">x‚ÇÅ‚ÇÄ</div>
                </div>
                <div class="diagram-arrow">‚Üí</div>
                <div class="diagram-box">
                    <h5>h‚ÇÅ‚ÇÄ</h5>
                    <div class="value">Output</div>
                </div>
            </div>
        </div>
        
        <p>Dans une couche RNN, on parcourt donc successivement les entr√©es x‚ÇÅ √† x‚Çú. √Ä l'instant t, la t√®me cellule combine l'entr√©e courante x‚Çú avec la pr√©diction au pas pr√©c√©dent h‚Çú‚Çã‚ÇÅ pour calculer une sortie h‚Çú de taille R.</p>
        
        <p>Le dernier vecteur calcul√© <strong>h‚Çú</strong> (qui est de taille R) est la sortie finale de la couche RNN. Une couche RNN d√©finit donc une relation de r√©currence de la forme :</p>
        
        <div class="formula">
            h‚Çú = f(x‚Çú, h‚Çú‚Çã‚ÇÅ)
        </div>
        
        <h3>Comportement d'une cellule</h3>
        
        <p>La t√®me cellule n'est rien d'autre qu'une couche dense de taille R, dont l'entr√©e est la concat√©nation de x‚Çú (de taille M=30) et h‚Çú‚Çã‚ÇÅ (de taille R). La fonction d'activation classique utilis√©e pour les cellules du RNN est la tangente hyperbolique <strong>tanh</strong>. Plusieurs propri√©t√©s int√©ressantes expliquent ce choix, notamment une convergence plus rapide que la sigmo√Øde.</p>
        
        <div class="architecture-diagram">
            <h4 style="text-align:center;margin-bottom:30px;">üîç Architecture d'une Cellule RNN</h4>
            <div class="rnn-cell-diagram">
                <div class="diagram-box">
                    <h5>x‚Çú</h5>
                    <div class="value">M=30</div>
                </div>
                <div class="diagram-arrow">+</div>
                <div class="diagram-box">
                    <h5>h‚Çú‚Çã‚ÇÅ</h5>
                    <div class="value">R=16</div>
                </div>
                <div class="diagram-arrow">‚Üí</div>
                <div class="diagram-box">
                    <h5>Concat</h5>
                    <div class="value">46</div>
                </div>
                <div class="diagram-arrow">‚Üí</div>
                <div class="diagram-box">
                    <h5>W·µÄ √ó</h5>
                    <div class="value">(46√ó16)</div>
                </div>
                <div class="diagram-arrow">‚Üí</div>
                <div class="diagram-box">
                    <h5>+ b</h5>
                    <div class="value">16</div>
                </div>
                <div class="diagram-arrow">‚Üí</div>
                <div class="diagram-box">
                    <h5>tanh</h5>
                    <div class="value">Activation</div>
                </div>
                <div class="diagram-arrow">‚Üí</div>
                <div class="diagram-box">
                    <h5>h‚Çú</h5>
                    <div class="value">R=16</div>
                </div>
            </div>
        </div>
        
        <p>La formule devient donc :</p>
        
        <div class="formula">
            h‚Çú = tanh( W·µÄ * concat(x‚Çú, h‚Çú‚Çã‚ÇÅ) + b )
        </div>
        
        <p>o√π W et b sont les poids appris par le mod√®le :</p>
        
        <div class="matrix-container">
            <div class="matrix">
                <div class="matrix-label">Matrice W</div>
                <div class="dimension-label">(R + M) √ó R = 46 √ó 16</div>
                <div style="padding:15px;color:#9ca3af;font-size:14px;">
                    Total: <strong style="color:#7dd3fc;">736 param√®tres</strong>
                </div>
            </div>
            <div class="matrix">
                <div class="matrix-label">Vecteur b</div>
                <div class="dimension-label">R = 16</div>
                <div style="padding:15px;color:#9ca3af;font-size:14px;">
                    Total: <strong style="color:#7dd3fc;">16 param√®tres</strong>
                </div>
            </div>
        </div>
        
        <div class="highlight-box">
            <p><strong>‚ö†Ô∏è Important :</strong> Les poids sont partag√©s entre toutes les cellules d'une couche RNN. Autrement dit, c'est exactement la m√™me fonction (avec les m√™mes poids) qui est appliqu√©e √† chaque pas de temps t. Cela permet √† la fois de mod√©liser toute la s√©quence de fa√ßon homog√®ne et d'√©viter de multiplier le nombre de poids √† apprendre. En effet, si on apprenait une matrice de poids diff√©rente pour chacune des T cellules, il faudrait alors apprendre T matrices (W‚ÇÅ, ..., W‚Çú) au lieu d'une seule.</p>
        </div>
        
        <p>Selon les impl√©mentations, il est possible que la matrice W soit s√©par√©e en deux parties :</p>
        
        <div class="comparison-grid">
            <div class="comparison-card">
                <h5>W‚Çï‚Çï</h5>
                <div class="dimension-label">R √ó R = 16 √ó 16</div>
                <p style="text-align:center;margin-top:10px;color:#9ca3af;">
                    Connexions entre<br>h‚Çú‚Çã‚ÇÅ et h‚Çú
                </p>
            </div>
            <div class="comparison-card">
                <h5>W‚Çì‚Çï</h5>
                <div class="dimension-label">M √ó R = 30 √ó 16</div>
                <p style="text-align:center;margin-top:10px;color:#9ca3af;">
                    Connexions entre<br>x‚Çú et h‚Çú
                </p>
            </div>
        </div>
        
        <p>C'est notamment le cas dans la librairie Keras que nous utilisons. Le code ci-dessous permet de d√©finir un mod√®le simple √† 1 couche RNN, o√π R=16, qui prend en entr√©e des s√©quences de taille T=10 caract√®res, chacun encod√© comme un vecteur de taille M=30.</p>
        
        <div class="code-block">
            <code>from keras.layers import SimpleRNN<br><br>model = Sequential()<br>model.add(SimpleRNN(units=16, input_shape=(10, 30), use_bias=False))</code>
        </div>
        
        <p>On voit alors que notre mod√®le a 736 param√®tres (ou poids) √† apprendre.</p>
        
        <div class="code-block">
            <code>print(model.summary())<br><br>_____________________________________________________________<br>Layer (type) Output Shape Param # <br>========================================================<br>simple_rnn_1 (SimpleRNN) (None, 16) 736 <br>========================================================<br>Total params: 736<br>Trainable params: 736<br>Non-trainable params: 0<br>_________________________________________________________________</code>
        </div>
        
        <p>On peut m√™me r√©cup√©rer les formes des matrices de poids apprises par le mod√®le.</p>
        
        <div class="code-block">
            <code>print([weight_matrix.shape for weight_matrix in model.get_weights()])<br><br>[(30, 16), (16, 16)]</code>
        </div>
        
        <p>Dans notre cas, on peut constater que le mod√®le apprend 2 matrices de poids :</p>
        
        <ul>
            <li>une matrice de taille <strong>(30 x 16) = (M x R)</strong> qui correspond √† W‚Çì‚Çï</li>
            <li>une matrice de taille <strong>(16 x 16) = (R x R)</strong> qui correspond √† W‚Çï‚Çï</li>
        </ul>
        
        <h3>Empiler des couches r√©currentes</h3>
        
        <p>Souvent, une seule couche RNN ne suffit pas √† capter toute l'information contenue dans les s√©quences. On peut alors empiler les couches de type RNN afin d'avoir un r√©seau plus profond, comme on le fait couramment avec des couches denses ou convolutives. Cela permet d'extraire des informations plus complexes √† partir des entr√©es, et ainsi d'avoir une meilleure mod√©lisation de nos donn√©es.</p>
        
        <div class="info-box">
            <p>Dans la plupart des cas, <strong>2 ou 5 couches r√©currentes suffisent</strong>; au-del√†, la convergence devient difficile. Cette "faible" profondeur est en r√©alit√© trompeuse, car les couches RNN sont en quelque sorte profondes par construction, vu qu'une couche unique traverse it√©rativement de longues s√©quences.</p>
        </div>
        
        <p>Essayons de construire un r√©seau √† 2 couches r√©currentes. La premi√®re couche RNN retourne un vecteur unique h‚ÇÅ‚Çú de taille R. La deuxi√®me couche, elle-m√™me r√©currente, a besoin d'une s√©quence de vecteurs en entr√©e, et non d'un vecteur unique.</p>
        
        <p>Comme fait-on alors pour satisfaire cette condition ? La solution consiste √† donner en entr√©e de la couche 2 la s√©quence des sorties interm√©diaires h‚ÇÅ‚ÇÅ, h‚ÇÅ‚ÇÇ, ..., h‚ÇÅ‚Çú de la couche 1, et pas seulement la sortie finale. Le sch√©ma de notre mod√®le est alors le suivant :</p>
        
        <div class="architecture-diagram">
            <h4 style="text-align:center;margin-bottom:30px;">üèóÔ∏è Empilement de 2 Couches RNN</h4>
            <div style="text-align:center;color:#9ca3af;margin-bottom:20px;">
                Couche 1: T=10 cellules ‚Üí h‚ÇÅ‚ÇÅ, h‚ÇÅ‚ÇÇ, ..., h‚ÇÅ‚ÇÅ‚ÇÄ
            </div>
            <div style="text-align:center;color:#7dd3fc;font-size:32px;margin:20px 0;">‚Üì</div>
            <div style="text-align:center;color:#9ca3af;margin-bottom:20px;">
                Couche 2: T=10 cellules ‚Üí h‚ÇÇ‚ÇÅ‚ÇÄ (sortie finale)
            </div>
        </div>
        
        <p>On peut impl√©menter un tel mod√®le avec Keras de la fa√ßon suivante :</p>
        
        <div class="code-block">
            <code>model = Sequential()<br>model.add(SimpleRNN(units=16, input_shape=(10,30), return_sequences=True))<br>model.add(SimpleRNN(units=4))</code>
        </div>
        
        <div class="highlight-box">
            <p>On peut remarquer dans le code ci-dessus que l'hyperparam√®tre "units", qu'on a not√© R dans notre article, peut √™tre diff√©rent d'une couche √† l'autre.</p>
        </div>
        
        <div class="section-divider"></div>
        
        <h2>Limites des RNN simples</h2>
        
        <p>Bien qu'ils soient construits sp√©cifiquement pour g√©rer des s√©quences, les RNN simples ont certaines limites.</p>
        
        <h3>Un mod√®le √† m√©moire courte</h3>
        
        <p>Une couche RNN est une succession de cellules, chacune prenant en entr√©e la repr√©sentation du caract√®re courant ainsi que la sortie de la cellule pr√©c√©dente (c.f sch√©ma paragraphe 2.A). Ces donn√©es d'entr√©e sont transform√©es, en passant notamment par une fonction tangente hyperbolique.</p>
        
        <p>La tangente hyperbolique a tendance √† √©craser les valeurs qu'elle prend en entr√©e. En effet, elle est d√©finie sur l'espace des r√©els mais ses valeurs de sortie sont dans l'intervalle ]-1, 1[. Ainsi, apr√®s un premier passage par la tangente hyperbolique, on obtient une valeur entre -1 et 1. Ensuite, comme tanh(1) = 0.76 et tanh(-1) = -0.76, un deuxi√®me passage par la fonction tanh r√©sulte en un intervalle de valeurs encore plus r√©duit, et ainsi de suite.</p>
        
        <div class="visualization-box">
            <h4>üìâ Comparaison des Fonctions tanh et Compositions Successives</h4>
            <div class="chart-container">
                <canvas id="tanhChart"></canvas>
            </div>
        </div>
        
        <p>Les informations provenant de x‚ÇÅ passent <strong>T fois √† travers une tangente hyperbolique</strong>, l√† o√π celles provenant de x‚Çú subissent cette transformation une seule fois. Cet effet d'√©crasement aurait pu √™tre compens√© par un poids plus fort associ√© √† h‚ÇÅ, via la matrice de poids W. Cependant, comme cette matrice est partag√©e par toutes les cellules de la couche, toutes les sorties interm√©diaires sont pond√©r√©es par les m√™mes poids. Intuitivement, <strong>l'influence de x‚ÇÅ dans la pr√©diction sera donc plus petite que celle de x‚Çú</strong>.</p>
        
        <div class="warning-box">
            <p><strong>Exemple :</strong> Supposons que notre s√©quence en entr√©e est "il pr√©vien". Pour pr√©dire la prochaine lettre, qui correspond √† l'accord du verbe, il est n√©cessaire de conna√Ætre le sujet. Or, cette information se trouve en d√©but de s√©quence. Le mod√®le aura potentiellement du mal √† diff√©rencier cette s√©quence d'une autre qui lui ressemble: "tu pr√©vien". Il pourrait donc donner une haute probabilit√© √† un "s", et ainsi se tromper. Ce probl√®me devient tr√®s emb√™tant lorsqu'on utilise des s√©quences plus longues, il est donc tr√®s compliqu√© pour un RNN basique de g√©n√©rer un texte coh√©rent dans la longueur.</p>
        </div>
        
        <h3>Un mod√®le difficile √† entra√Æner</h3>
        
        <p>L'une des principales difficult√©s pour entra√Æner des r√©seaux de type RNN est le probl√®me du <strong>vanishing gradient</strong>. Pour comprendre ce probl√®me, faisons tout d'abord quelques rappels au sujet de l'entra√Ænement d'un r√©seau de neurones. Un r√©seau de neurones est un encha√Ænement de fonctions simples, prenant en entr√©e les donn√©es de notre probl√®me X. Dans notre exemple de g√©n√©ration de texte, il s'agit des N s√©quences de taille T.</p>
        
        <p>Cet encha√Ænement de fonctions a comme param√®tres des poids W. L'apprentissage consiste √† ajuster les poids W afin de minimiser une erreur, donn√©e par une fonction de co√ªt L. Cette erreur mesure l'√©cart entre les labels r√©els y (la "v√©rit√©", les caract√®res que le mod√®le est cens√© pr√©dire) et les labels pr√©dits ≈∑ (les caract√®res pr√©dits par le mod√®le). Elle est mesur√©e sur une partie de nos donn√©es (X_test, y_test).</p>
        
        <p>L'entra√Ænement a lieu comme il suit :</p>
        
        <ol>
            <li>Les poids W sont initialis√©s al√©atoirement</li>
            <li>On fait passer X_train par notre mod√®le pour obtenir une pr√©diction ≈∑_train. On calcule ensuite la valeur de la fonction de co√ªt L(y_train, ≈∑_train)</li>
            <li>On calcule le gradient de cette fonction L par rapport aux param√®tres W</li>
            <li>On cherche √† minimiser notre fonction de co√ªt en mettant √† jour les param√®tres W. Pour cela, on effectue une descente de gradient gr√¢ce au gradient calcul√© √† l'√©tape pr√©c√©dente : <strong>W = W - Œª grad L</strong></li>
        </ol>
        
        <p>Un r√©seau de neurones est une suite de fonctions (couches) appliqu√©es successivement. Calculer son gradient revient donc √† d√©river des fonctions compos√©es. Sch√©matiquement, cela revient √† calculer la d√©riv√©e d'une fonction compos√©e comme une multiplication de d√©riv√©es partielles des fonctions qui la composent.</p>
        
        <p>Ainsi, si notre mod√®le est compos√© de trois couches et est repr√©sent√© par la fonction f ‚àò g ‚àò h, son gradient par rapport √† W sera <strong>df/dg * dg/dh * dh/dW</strong>.</p>
        
        <p>Dans le cas d'une couche RNN, on a :</p>
        
        <div class="formula">
            h‚ÇÅ = f_W(x‚ÇÅ, h‚ÇÄ)<br>
            h‚ÇÇ = f_W(x‚ÇÇ, h‚ÇÅ) = f_W(x‚ÇÇ, f_W(x‚ÇÅ, h‚ÇÄ))<br>
            h‚ÇÉ = f_W(x‚ÇÉ, h‚ÇÇ) = f_W(x‚ÇÉ, f_W(x‚ÇÇ, f_W(x‚ÇÅ, h‚ÇÄ)))
        </div>
        
        <p>avec f_W(x, h) = tanh(W·µÄ concat(x, h) + b) et une fonction de co√ªt L(h‚ÇÉ, y).</p>
        
        <p>Le gradient de h‚ÇÉ par rapport √† W est donc proportionnel √† la multiplication de d√©riv√©es de la fonction tangente hyperbolique. Or, la d√©riv√©e de cette fonction se situe dans l'intervalle [0, 1] et prend presque s√ªrement des valeurs dans [0, 1[.</p>
        
        <div class="visualization-box">
            <h4>üìä D√©riv√©e de tanh</h4>
            <div class="chart-container">
                <canvas id="tanhDerivativeChart"></canvas>
            </div>
        </div>
        
        <div class="visualization-box">
            <h4>‚ö†Ô∏è Visualisation de la Disparition du Gradient</h4>
            <div class="chart-container">
                <canvas id="gradientChart"></canvas>
            </div>
        </div>
        
        <div class="warning-box">
            <p><strong>Le probl√®me :</strong> Plus on multiplie des valeurs entre 0 et 1 entre elles, plus le r√©sultat se rapproche de 0. dL/dW prend donc des valeurs tr√®s petites lorsque les s√©quences sont longues. La mise √† jour des param√®tres devient donc tr√®s lente et l'entra√Ænement du mod√®le est mis √† mal.</p>
        </div>
        
        <div class="section-divider"></div>
        
        <h2>Le LSTM : un RNN am√©lior√©</h2>
        
        <h3>Intuition derri√®re l'architecture LSTM</h3>
        
        <p>Plusieurs variantes aux RNN standards ont vu le jour pour rem√©dier aux probl√®mes √©voqu√©s pr√©c√©demment. Nous allons ici d√©crire les <strong>LSTM</strong>, pour <strong>Long Short-Term Memory</strong>. Ce type de RNN est tr√®s utilis√© en traitement du langage naturel.</p>
        
        <p>L'id√©e derri√®re ce choix d'architecture de r√©seaux de neurones est de diviser le signal entre ce qui est important √† court terme √† travers le <strong>hidden state</strong> (analogue √† la sortie d'une cellule de RNN simple), et ce qui l'est √† long terme, √† travers le <strong>cell state</strong>, qui sera explicit√© plus bas. Ainsi, le fonctionnement global d'un LSTM peut se r√©sumer en 3 √©tapes :</p>
        
        <div class="comparison-grid">
            <div class="comparison-card">
                <h5>1Ô∏è‚É£ Forget Gate</h5>
                <p style="text-align:center;color:#9ca3af;">
                    D√©tecte les informations pertinentes du pass√©
                </p>
            </div>
            <div class="comparison-card">
                <h5>2Ô∏è‚É£ Input Gate</h5>
                <p style="text-align:center;color:#9ca3af;">
                    Choisit les nouvelles informations √† stocker
                </p>
            </div>
            <div class="comparison-card">
                <h5>3Ô∏è‚É£ Output Gate</h5>
                <p style="text-align:center;color:#9ca3af;">
                    G√©n√®re le hidden state √† partir du cell state
                </p>
            </div>
        </div>
        
        <p>Comme le RNN, le LSTM d√©finit donc une relation de r√©currence, mais utilise une variable suppl√©mentaire qui est le <strong>cell state c</strong> :</p>
        
        <div class="formula">
            h‚Çú, c‚Çú = f(x‚Çú, h‚Çú‚Çã‚ÇÅ, c‚Çú‚Çã‚ÇÅ)
        </div>
        
        <p>L'information transite d'une cellule √† la suivante par deux canaux, h et c. √Ä l'instant t, ces deux canaux se mettent √† jour par l'interaction entre leurs valeurs pr√©c√©dentes h‚Çú‚Çã‚ÇÅ et c‚Çú‚Çã‚ÇÅ ainsi que l'√©l√©ment courant de la s√©quence x‚Çú.</p>
        
        <div class="architecture-diagram">
            <h4 style="text-align:center;margin-bottom:30px;">üß† Architecture LSTM vs RNN</h4>
            <div class="comparison-grid">
                <div class="comparison-card">
                    <h5>RNN Simple</h5>
                    <div style="margin:20px 0;">
                        <div class="diagram-box" style="margin:10px auto;max-width:200px;">
                            <h5>h‚Çú‚Çã‚ÇÅ</h5>
                            <div class="diagram-arrow">‚Üí</div>
                            <h5>x‚Çú</h5>
                            <div class="diagram-arrow">‚Üí</div>
                            <h5>h‚Çú</h5>
                        </div>
                    </div>
                    <p style="text-align:center;color:#9ca3af;font-size:14px;">
                        1 canal de m√©moire
                    </p>
                </div>
                <div class="comparison-card">
                    <h5>LSTM</h5>
                    <div style="margin:20px 0;">
                        <div class="diagram-box" style="margin:10px auto;max-width:200px;">
                            <h5>h‚Çú‚Çã‚ÇÅ, c‚Çú‚Çã‚ÇÅ</h5>
                            <div class="diagram-arrow">‚Üí</div>
                            <h5>x‚Çú</h5>
                            <div class="diagram-arrow">‚Üí</div>
                            <h5>h‚Çú, c‚Çú</h5>
                        </div>
                    </div>
                    <p style="text-align:center;color:#9ca3af;font-size:14px;">
                        2 canaux : hidden + cell state
                    </p>
                </div>
            </div>
        </div>
        
        <h3>La cellule LSTM pas √† pas</h3>
        
        <ol class="steps-list">
            <li>
                <strong>La t√®me cellule LSTM prend 3 vecteurs en entr√©e :</strong><br>
                ‚Ä¢ L'√©l√©ment courant de la s√©quence x‚Çú : repr√©sentation vectorielle du t√®me caract√®re (vecteur de taille M)<br>
                ‚Ä¢ Le hidden state de la cellule pr√©c√©dente h‚Çú‚Çã‚ÇÅ (vecteur de taille R)<br>
                ‚Ä¢ Le cell state de la cellule pr√©c√©dente c‚Çú‚Çã‚ÇÅ (vecteur de taille R).<br><br>
                C'est ce dernier vecteur que nous allons suivre particuli√®rement. Il s'agit d'une route privil√©gi√©e de transmission d'information sur la s√©quence. Pour √©viter le probl√®me du vanishing gradient, le cell state est en effet mis √† jour de fa√ßon additive √† chaque √©tape, sans passer par une activation.
            </li>
            
            <li>
                <strong>La forget gate</strong> est une couche dense de taille R avec une activation sigmo√Øde. √Ä partir de h‚Çú‚Çã‚ÇÅ et x‚Çú, cette forget gate produit un vecteur f‚Çú de taille R, dont les valeurs sont comprises entre 0 et 1.
            </li>
            
            <li>
                <strong>La forget gate agit comme un filtre</strong> pour "oublier" certaines informations du cell state. En effet, on effectue une multiplication terme √† terme entre f‚Çú et c‚Çú‚Çã‚ÇÅ, ce qui a tendance √† annuler les composantes de c‚Çú‚Çã‚ÇÅ dont les homologues c√¥t√© f‚Çú sont proches de 0. On obtient alors un cell state filtr√©, toujours de taille R.
            </li>
            
            <li>
                <strong>L'input gate</strong> produit un filtre i‚Çú de valeurs comprises entre 0 et 1 et de taille R, √† partir de h‚Çú‚Çã‚ÇÅ et x‚Çú, de fa√ßon similaire √† la forget gate.
            </li>
            
            <li>
                En parall√®le, un vecteur <strong>k‚Çú</strong> de taille R est cr√©√© par une couche tanh. k‚Çú est le vecteur candidat pour mettre √† jour le cell state.
            </li>
            
            <li>
                Le candidat k‚Çú est filtr√© par l'input gate i‚Çú via une multiplication terme √† terme. On obtient le vecteur de mise √† jour du cell state.
            </li>
            
            <li>
                Le cell state filtr√© (obtenu √† l'√©tape 3) est mis √† jour gr√¢ce au vecteur candidat filtr√© (obtenu √† l'√©tape 6). La mise √† jour est une simple addition terme √† terme de ces deux vecteurs. On obtient alors le nouveau <strong>cell state c‚Çú</strong>.
            </li>
            
            <li>
                De fa√ßon analogue √† f‚Çú et i‚Çú, l'<strong>output gate</strong> produit un filtre o‚Çú de valeurs entre 0 et 1, et de taille R.
            </li>
            
            <li>
                Les valeurs du nouveau cell state c‚Çú sont ramen√©es √† l'intervalle ]-1, 1[ par une activation tanh. Un filtrage par l'output gate o‚Çú est ensuite effectu√© pour enfin obtenir la sortie <strong>h‚Çú</strong>.
            </li>
        </ol>
        
        <div class="section-divider"></div>
        
        <h3>Quelques astuces pour une meilleure performance</h3>
        
        <p>Des recherches sur les architectures RNN pour les t√¢ches de traduction ont d√©montr√© que le sens de lecture classique des s√©quences n'est pas optimal. En effet, ils ont montr√© empiriquement que lire les s√©quences √† l'envers am√©liore significativement la performance du mod√®le dans ce genre de probl√®mes. La combinaison des deux sens de lecture (<strong>bidirectional RNN</strong>) donne des r√©sultats encore meilleurs. Ce dernier r√©sultat est pr√©visible, vu qu'une repr√©sentation bidirectionnelle permet au mod√®le d'accorder autant d'importance aux premiers caract√®res de la s√©quence qu'aux derniers. La version unidirectionnelle, quant √† elle, "dilue" m√©caniquement les premiers caract√®res, m√™me si ce ph√©nom√®ne est moins prononc√© dans un LSTM que dans un RNN simple.</p>
        
        <div class="section-divider"></div>
        
        <h2>Les mod√®les d'attention : une nouvelle fa√ßon d'apprendre des s√©quences</h2>
        
        <p>√Ä travers cet article, nous avons tent√© d'expliquer le fonctionnement interne des RNN, et d'une de leurs variantes les plus utilis√©es, le LSTM. Ces structures, construites pour traiter des donn√©es s√©quentielles, sont √† la base de grandes avanc√©es en NLP, que ce soit pour la mod√©lisation du langage, la traduction ou le r√©sum√© automatique de texte.</p>
        
        <p>Bien qu'efficace, cette approche a quand m√™me quelques inconv√©nients. L'un des d√©fauts des LSTM est qu'il est n√©cessaire de lire enti√®rement une s√©quence pour produire une pr√©diction. En traduction par exemple, cette d√©marche reviendrait √† lire enti√®rement une phrase en m√©morisant tous ses mots, pour ensuite produire une phrase traduite d'un seul coup. On imagine mal un humain proc√©der de la m√™me fa√ßon.</p>
        
        <div class="info-box">
            <p>Les architectures utilisant le m√©canisme d'<strong>attention</strong> r√©pondent √† cette limitation des LSTM. Elles suivent l'intuition que tous les mots n'ont pas le m√™me poids s√©mantique dans la production d'une phrase traduite, et qu'on ne traduit jamais mot √† mot. En se focalisant sur des parties de la phrase, et en identifiant un contexte "utile" pour chaque mot, ces m√©canismes permettent d'am√©liorer encore plus les performances des mod√®les.</p>
        </div>
        
        <p>Lors de notre prochain article, nous analyserons certains mod√®les d'attention et t√¢cherons d'expliquer leur apport en NLP.</p>
        
        <div class="section-divider"></div>
        
        <div style="text-align:center;margin-top:50px;padding:30px;background:rgba(125,211,252,0.1);border-radius:10px;">
            <p style="color:#7dd3fc;font-size:18px;">üìö Article r√©dig√© par <strong>Sofiene Alouini</strong> et <strong>Sofia Calcagno</strong></p>
            <p style="margin-top:15px;">
                <a href="index.html" style="color:#7dd3fc;text-decoration:none;padding:10px 20px;background:rgba(125,211,252,0.2);border-radius:8px;display:inline-block;margin-top:15px;">‚Üê Retour aux visualisations</a>
            </p>
        </div>
    </div>
</div>

<script>
// Configuration Chart.js pour th√®me sombre
Chart.defaults.color = '#e5e7eb';
Chart.defaults.borderColor = 'rgba(125,211,252,0.2)';
Chart.defaults.backgroundColor = 'rgba(125,211,252,0.1)';

// Graphique tanh et compositions successives
const tanhCtx = document.getElementById('tanhChart').getContext('2d');
const tanhData = {
    labels: Array.from({length: 201}, (_, i) => ((i - 100) / 20).toFixed(1)),
    datasets: [
        {
            label: 'tanh(x)',
            data: Array.from({length: 201}, (_, i) => {
                const x = (i - 100) / 20;
                return Math.tanh(x);
            }),
            borderColor: '#3b82f6',
            backgroundColor: 'rgba(59,130,246,0.1)',
            borderWidth: 3,
            fill: true
        },
        {
            label: 'tanh(tanh(x))',
            data: Array.from({length: 201}, (_, i) => {
                const x = (i - 100) / 20;
                return Math.tanh(Math.tanh(x));
            }),
            borderColor: '#10b981',
            backgroundColor: 'rgba(16,185,129,0.1)',
            borderWidth: 3,
            fill: true
        },
        {
            label: 'tanh(tanh(tanh(x)))',
            data: Array.from({length: 201}, (_, i) => {
                const x = (i - 100) / 20;
                return Math.tanh(Math.tanh(Math.tanh(x)));
            }),
            borderColor: '#f59e0b',
            backgroundColor: 'rgba(245,158,11,0.1)',
            borderWidth: 3,
            fill: true
        }
    ]
};

new Chart(tanhCtx, {
    type: 'line',
    data: tanhData,
    options: {
        responsive: true,
        maintainAspectRatio: false,
        plugins: {
            legend: {
                labels: {
                    color: '#e5e7eb',
                    font: {size: 14}
                }
            },
            title: {
                display: true,
                text: 'Effet d\'√©crasement des compositions successives de tanh',
                color: '#7dd3fc',
                font: {size: 16, weight: 'bold'}
            }
        },
        scales: {
            x: {
                title: {
                    display: true,
                    text: 'x',
                    color: '#9ca3af'
                },
                grid: {
                    color: 'rgba(125,211,252,0.1)'
                }
            },
            y: {
                title: {
                    display: true,
                    text: 'y',
                    color: '#9ca3af'
                },
                min: -1.2,
                max: 1.2,
                grid: {
                    color: 'rgba(125,211,252,0.1)'
                }
            }
        }
    }
});

// Graphique d√©riv√©e de tanh
const tanhDerivativeCtx = document.getElementById('tanhDerivativeChart').getContext('2d');
const tanhDerivativeData = {
    labels: Array.from({length: 201}, (_, i) => ((i - 100) / 20).toFixed(1)),
    datasets: [{
        label: 'tanh\'(x) = 1 - tanh¬≤(x)',
        data: Array.from({length: 201}, (_, i) => {
            const x = (i - 100) / 20;
            const tanhX = Math.tanh(x);
            return 1 - tanhX * tanhX;
        }),
        borderColor: '#ef4444',
        backgroundColor: 'rgba(239,68,68,0.1)',
        borderWidth: 3,
        fill: true
    }]
};

new Chart(tanhDerivativeCtx, {
    type: 'line',
    data: tanhDerivativeData,
    options: {
        responsive: true,
        maintainAspectRatio: false,
        plugins: {
            legend: {
                labels: {
                    color: '#e5e7eb',
                    font: {size: 14}
                }
            },
            title: {
                display: true,
                text: 'D√©riv√©e de tanh : valeurs entre 0 et 1',
                color: '#7dd3fc',
                font: {size: 16, weight: 'bold'}
            }
        },
        scales: {
            x: {
                title: {
                    display: true,
                    text: 'x',
                    color: '#9ca3af'
                },
                grid: {
                    color: 'rgba(125,211,252,0.1)'
                }
            },
            y: {
                title: {
                    display: true,
                    text: 'tanh\'(x)',
                    color: '#9ca3af'
                },
                min: -0.1,
                max: 1.1,
                grid: {
                    color: 'rgba(125,211,252,0.1)'
                }
            }
        }
    }
});

// Graphique disparition du gradient
const gradientCtx = document.getElementById('gradientChart').getContext('2d');
const timesteps = Array.from({length: 20}, (_, i) => `t=${i}`);
const gradientValues = Array.from({length: 20}, (_, i) => {
    const factor = 0.25; // Chaque timestep r√©duit le gradient de 75%
    return Math.pow(factor, i) * 100;
});

const gradientData = {
    labels: timesteps,
    datasets: [{
        label: 'Gradient (%)',
        data: gradientValues,
        borderColor: '#ef4444',
        backgroundColor: 'rgba(239,68,68,0.2)',
        borderWidth: 3,
        fill: true,
        tension: 0.4
    }]
};

new Chart(gradientCtx, {
    type: 'line',
    data: gradientData,
    options: {
        responsive: true,
        maintainAspectRatio: false,
        plugins: {
            legend: {
                labels: {
                    color: '#e5e7eb',
                    font: {size: 14}
                }
            },
            title: {
                display: true,
                text: 'Disparition exponentielle du gradient lors de la backpropagation',
                color: '#7dd3fc',
                font: {size: 16, weight: 'bold'}
            }
        },
        scales: {
            x: {
                title: {
                    display: true,
                    text: 'Timestep',
                    color: '#9ca3af'
                },
                grid: {
                    color: 'rgba(125,211,252,0.1)'
                }
            },
            y: {
                title: {
                    display: true,
                    text: 'Gradient (%)',
                    color: '#9ca3af'
                },
                min: 0,
                max: 110,
                grid: {
                    color: 'rgba(125,211,252,0.1)'
                }
            }
        }
    }
});
</script>

</body>
</html>
